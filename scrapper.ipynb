{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file: str = 'urls.txt'\n",
    "with open(file, 'r') as f:\n",
    "    urls = f.readlines()\n",
    "    webs = [url.strip() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define client headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"Website\", \"Images\", \"Gifs\", \"Title Size\", \"Links\", \"Languages\", \"h1\", \"h2\", \"h3\", \"HTTPS\", \"Paragraphs\", \"Meta Description\", \"Meta Keywords\", \"JS Scripts\", \"Size KB\", \"Words\", \"Cookies\", \"MENA\"]\n",
    "df = pd.DataFrame(columns=variables)\n",
    "df[\"Website\"] = webs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numImages(soup):\n",
    "    return len(soup.find_all('img'))\n",
    "\n",
    "def get_numGifs(soup):\n",
    "    return len(soup.find_all('gif'))\n",
    "\n",
    "def is_https(url):\n",
    "    return url.startswith('https')\n",
    "\n",
    "def get_titleSize(soup):\n",
    "    try:\n",
    "        return len(soup.title.string)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_numLinks(soup):\n",
    "    return len(soup.find_all('a'))\n",
    "\n",
    "def get_languages(soup):\n",
    "    try:\n",
    "        return len(soup.html[\"lang\"])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_numH1(soup):\n",
    "    return len(soup.find_all('h1'))\n",
    "\n",
    "def get_numH2(soup):\n",
    "    return len(soup.find_all('h2'))\n",
    "\n",
    "def get_numH3(soup):\n",
    "    return len(soup.find_all('h3'))\n",
    "\n",
    "def get_Paragraphs(soup):\n",
    "    return len(soup.find_all('p'))\n",
    "\n",
    "def has_metadescription(soup):\n",
    "    return len(soup.find_all('meta', attrs={'name': 'description'})) > 0\n",
    "\n",
    "def has_metakeywords(soup):\n",
    "    return len(soup.find_all('meta', attrs={'name': 'keywords'})) > 0\n",
    "\n",
    "def javascript_size(soup):\n",
    "    return sum([len(script) for script in soup.find_all('script')])\n",
    "\n",
    "def num_words(soup):\n",
    "    return len(soup.text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  403  for  https://www.surfer.com/\n",
      "Error:  403  for  https://www.sportspromedia.com/\n",
      "Error:  403  for  https://www.fei.org/\n",
      "Error:  403  for  https://www.nascar.com/\n",
      "Error: Could not connect to  https://www.worldsquash.org/\n",
      "Error:  403  for  https://www.usatoday.com/sports/\n",
      "Error: Could not connect to  https://www.kingfut.com/\n",
      "Error: Could not connect to  https://www.gdnonline.com/\n"
     ]
    }
   ],
   "source": [
    "for i, web in enumerate(webs):\n",
    "    try:\n",
    "        response = requests.get(web, headers=headers)\n",
    "    except:\n",
    "        print(\"Error: Could not connect to \", web)\n",
    "        continue\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: \", response.status_code, \" for \", web)\n",
    "        continue\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    df.loc[df[\"Website\"] == web, \"Images\"] = get_numImages(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Gifs\"] = get_numGifs(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"HTTPS\"] = is_https(web)\n",
    "    df.loc[df[\"Website\"] == web, \"Title Size\"] = get_titleSize(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Links\"] = get_numLinks(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Languages\"] = get_languages(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"h1\"] = get_numH1(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"h2\"] = get_numH2(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"h3\"] = get_numH3(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Paragraphs\"] = get_Paragraphs(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Meta Description\"] = has_metadescription(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Meta Keywords\"] = has_metakeywords(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Size KB\"] = len(response.content) / 1024\n",
    "    df.loc[df[\"Website\"] == web, \"JS Scripts\"] = javascript_size(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Words\"] = num_words(soup)\n",
    "    df.loc[df[\"Website\"] == web, \"Cookies\"] = 'Set-Cookie' in response.headers\n",
    "    df.loc[df[\"Website\"] == web, \"MENA\"] = \"TRUE\" if i >= 91 else \"FALSE\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./PageSpeed/PageSpeed_final.csv')\n",
    "df2.drop(['Unnamed: 0', 'index'], axis=1, inplace=True)\n",
    "\n",
    "# Join the two dataframes by website (df2) and Link (df)\n",
    "df3 = pd.merge(df, df2, left_on='Website', right_on='Link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "df3.dropna(inplace=True)\n",
    "df3.drop(['Link'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ps = {'LCP_m':'s', 'LCP_o':'s', 'INP_m':'ms', 'INP_o':'ms', 'FCP_m':'s', 'FCP_o':'s', 'TTF_m':'s', 'TTF_o':'s', 'CLS_m':'', 'CLS_o':'', 'FID_m':'ms', 'FID_o':'ms'}\n",
    "for var in variables_ps:\n",
    "    df3[var] = df3[var].apply(lambda x: float(str(x).split(' ')[0]))\n",
    "df3[variables_ps.keys()]\n",
    "\n",
    "for var, v in variables_ps.items():\n",
    "    # rename var in df3\n",
    "    if v != '':\n",
    "        df3.rename(columns={var: var + f'({v})'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('final_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
